From 9b37ea9ab1ca76785bfe12c1fa2536faddb1e9fb Mon Sep 17 00:00:00 2001
From: Juri Lelli <juri.lelli@arm.com>
Date: Tue, 7 Jul 2015 11:24:00 -0700
Subject: [PATCH 06/19] sched/{core,fair}: trigger OPP change request on fork()

Patch "sched/fair: add triggers for OPP change requests" introduced OPP
change triggers for enqueue_task_fair(), but the trigger was operating only
for wakeups. Fact is that it makes sense to consider wakeup_new also (i.e.,
fork()), as we don't know anything about a newly created task and thus we
most certainly want to jump to max OPP to not harm performance too much.

However, it is not currently possible (or at least it wasn't evident to me
how to do so :/) to tell new wakeups from other (non wakeup) operations.

This patch introduces an additional flag in sched.h that is only set at
fork() time and it is then consumed in enqueue_task_fair() for our purpose.

cc: Ingo Molnar <mingo@redhat.com>
cc: Peter Zijlstra <peterz@infradead.org>

Signed-off-by: Juri Lelli <juri.lelli@arm.com>
---
 kernel/sched/core.c  | 2 +-
 kernel/sched/fair.c  | 8 +++-----
 kernel/sched/sched.h | 1 +
 3 files changed, 5 insertions(+), 6 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 67f5d0e..abc81af 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2355,7 +2355,7 @@ void wake_up_new_task(struct task_struct *p)
 #endif
 
 	rq = __task_rq_lock(p);
-	activate_task(rq, p, 0);
+	activate_task(rq, p, ENQUEUE_WAKEUP_NEW);
 	p->on_rq = TASK_ON_RQ_QUEUED;
 	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq, p, WF_FORK);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 9469dd7..c6e28a5 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4134,7 +4134,8 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 {
 	struct cfs_rq *cfs_rq;
 	struct sched_entity *se = &p->se;
-	int task_new = !(flags & ENQUEUE_WAKEUP);
+	int task_new = flags & ENQUEUE_WAKEUP_NEW;
+	int task_wakeup = flags & ENQUEUE_WAKEUP;
 
 	for_each_sched_entity(se) {
 		if (se->on_rq)
@@ -4177,14 +4178,11 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		 * load balancing, but in these cases it seems wise to trigger
 		 * as single request after load balancing is done.
 		 *
-		 * XXX: how about fork()? Do we need a special flag/something
-		 *      to tell if we are here after a fork() (wakeup_task_new)?
-		 *
 		 * Also, we add a margin (same ~20% used for the tipping point)
 		 * to our request to provide some head room if p's utilization
 		 * further increases.
 		 */
-		if (sched_energy_freq() && !task_new) {
+		if (sched_energy_freq() && (task_new || task_wakeup)) {
 			unsigned long req_cap = get_cpu_usage(cpu_of(rq));
 
 			req_cap = req_cap * capacity_margin
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 41bb2c9..f4f5245 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1169,6 +1169,7 @@ static const u32 prio_to_wmult[40] = {
 #define ENQUEUE_WAKING		0
 #endif
 #define ENQUEUE_REPLENISH	8
+#define ENQUEUE_WAKEUP_NEW	16
 
 #define DEQUEUE_SLEEP		1
 
-- 
1.9.1

