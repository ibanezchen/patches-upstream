From 157859171b6509cc9fa8b58b4b8aedf2f2befd89 Mon Sep 17 00:00:00 2001
From: Steve Muckle <steve.muckle@linaro.org>
Date: Tue, 6 Oct 2015 16:26:24 -0700
Subject: [PATCH 11/19] sched/sched-dvfs: fix references to get_cpu_usage()

get_cpu_usage() has been renamed to cpu_util()
---
 kernel/sched/fair.c | 18 +++++++++---------
 1 file changed, 9 insertions(+), 9 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 66de669..5cebd37 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4121,7 +4121,7 @@ static inline void hrtick_update(struct rq *rq)
 static unsigned int capacity_margin = 1280; /* ~20% margin */
 
 static bool cpu_overutilized(int cpu);
-static unsigned long get_cpu_usage(int cpu);
+static unsigned long cpu_util(int cpu);
 struct static_key __sched_energy_freq __read_mostly = STATIC_KEY_INIT_FALSE;
 
 /*
@@ -4183,7 +4183,7 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		 * further increases.
 		 */
 		if (sched_energy_freq() && (task_new || task_wakeup)) {
-			unsigned long req_cap = get_cpu_usage(cpu_of(rq));
+			unsigned long req_cap = cpu_util(cpu_of(rq));
 
 			req_cap = req_cap * capacity_margin
 					>> SCHED_CAPACITY_SHIFT;
@@ -4260,7 +4260,7 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		 * further increases.
 		 */
 		if (sched_energy_freq() && task_sleep) {
-			unsigned long req_cap = get_cpu_usage(cpu_of(rq));
+			unsigned long req_cap = cpu_util(cpu_of(rq));
 
 			if (rq->cfs.nr_running) {
 				req_cap = req_cap * capacity_margin
@@ -7548,7 +7548,7 @@ more_balance:
 		 * tasks.
 		 */
 		if (sched_energy_freq() && cur_ld_moved) {
-			unsigned long req_cap = get_cpu_usage(env.src_cpu);
+			unsigned long req_cap = cpu_util(env.src_cpu);
 
 			req_cap = req_cap * capacity_margin
 					>> SCHED_CAPACITY_SHIFT;
@@ -7577,7 +7577,7 @@ more_balance:
 			 */
 			if (sched_energy_freq()) {
 				unsigned long req_cap =
-					get_cpu_usage(env.dst_cpu);
+					cpu_util(env.dst_cpu);
 
 				req_cap = req_cap * capacity_margin
 						>> SCHED_CAPACITY_SHIFT;
@@ -7949,7 +7949,7 @@ static int active_load_balance_cpu_stop(void *data)
 			 */
 			if (sched_energy_freq()) {
 				unsigned long req_cap =
-					get_cpu_usage(env.src_cpu);
+					cpu_util(env.src_cpu);
 
 				req_cap = req_cap * capacity_margin
 						>> SCHED_CAPACITY_SHIFT;
@@ -7974,7 +7974,7 @@ out_unlock:
 		 * further increases.
 		 */
 		if (sched_energy_freq()) {
-			unsigned long req_cap = get_cpu_usage(target_cpu);
+			unsigned long req_cap = cpu_util(target_cpu);
 
 			req_cap = req_cap * capacity_margin
 					>> SCHED_CAPACITY_SHIFT;
@@ -8469,7 +8469,7 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 	/*
 	 * To make free room for a task that is building up its "real"
 	 * utilization and to harm its performance the least, request a
-	 * jump to max OPP as soon as get_cpu_usage() crosses the UP
+	 * jump to max OPP as soon as cpu_util() crosses the UP
 	 * threshold. The UP threshold is built relative to the current
 	 * capacity (OPP), by using same margin used to tell if a cpu
 	 * is overutilized (capacity_margin).
@@ -8481,7 +8481,7 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 
 		if (capacity_curr < capacity_orig &&
 		    (capacity_curr * SCHED_LOAD_SCALE) <
-		    (get_cpu_usage(cpu) * capacity_margin))
+		    (cpu_util(cpu) * capacity_margin))
 			cpufreq_sched_set_cap(cpu, capacity_orig);
 	}
 }
-- 
1.9.1

