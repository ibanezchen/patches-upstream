From 5d6ee98e825e76dd065c9c4ebf9c7b61b763c03f Mon Sep 17 00:00:00 2001
From: Steve Muckle <steve.muckle@linaro.org>
Date: Tue, 13 Oct 2015 13:21:14 -0700
Subject: [PATCH 13/19] sched/cpufreq_sched: communicate normalized capacities
 to cpufreq_sched

Fix things for heterogeneous CPU topologies within a single frequency
domain. Without this change, requests on smaller CPUs for higher OPPs
may be ignored due to requests for lower OPPs on bigger CPUs, because
the latter has a higher global capacity.

Signed-off-by: Steve Muckle <steve.muckle@linaro.org>
---
 kernel/sched/cpufreq_sched.c |  5 +++--
 kernel/sched/fair.c          | 14 +++++++-------
 2 files changed, 10 insertions(+), 9 deletions(-)

diff --git a/kernel/sched/cpufreq_sched.c b/kernel/sched/cpufreq_sched.c
index b81ac779..c5c52b1 100644
--- a/kernel/sched/cpufreq_sched.c
+++ b/kernel/sched/cpufreq_sched.c
@@ -128,7 +128,8 @@ static void cpufreq_sched_irq_work(struct irq_work *irq_work)
 /**
  * cpufreq_sched_set_capacity - interface to scheduler for changing capacity values
  * @cpu: cpu whose capacity utilization has recently changed
- * @capacity: the new capacity requested by cpu
+ * @capacity: the new capacity requested by cpu, normalized to that cpu, in the
+ *            range of 1 - SCHED_CAPACITY_SCALE
  *
  * cpufreq_sched_sched_capacity is an interface exposed to the scheduler so
  * that the scheduler may inform the governor of updates to capacity
@@ -184,7 +185,7 @@ void cpufreq_sched_set_cap(int cpu, unsigned long capacity)
 		goto out;
 
 	/* Convert the new maximum capacity request into a cpu frequency */
-	freq_new = (capacity * policy->max) / capacity_orig_of(cpu);
+	freq_new = capacity * policy->max >> SCHED_CAPACITY_SHIFT;
 
 	/* No change in frequency? Bail and return current capacity. */
 	if (freq_new == policy->cur)
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 5cebd37..bf4da6f 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4186,7 +4186,7 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 			unsigned long req_cap = cpu_util(cpu_of(rq));
 
 			req_cap = req_cap * capacity_margin
-					>> SCHED_CAPACITY_SHIFT;
+				/ capacity_orig_of(cpu_of(rq));
 			cpufreq_sched_set_cap(cpu_of(rq), req_cap);
 		}
 	}
@@ -4264,7 +4264,7 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 			if (rq->cfs.nr_running) {
 				req_cap = req_cap * capacity_margin
-						>> SCHED_CAPACITY_SHIFT;
+					/ capacity_orig_of(cpu_of(rq));
 				cpufreq_sched_set_cap(cpu_of(rq), req_cap);
 			} else {
 				cpufreq_sched_reset_cap(cpu_of(rq));
@@ -7551,7 +7551,7 @@ more_balance:
 			unsigned long req_cap = cpu_util(env.src_cpu);
 
 			req_cap = req_cap * capacity_margin
-					>> SCHED_CAPACITY_SHIFT;
+				/ capacity_orig_of(env.src_cpu);
 			cpufreq_sched_set_cap(env.src_cpu, req_cap);
 		}
 
@@ -7580,7 +7580,7 @@ more_balance:
 					cpu_util(env.dst_cpu);
 
 				req_cap = req_cap * capacity_margin
-						>> SCHED_CAPACITY_SHIFT;
+					/ capacity_orig_of(env.dst_cpu);
 				cpufreq_sched_set_cap(env.dst_cpu, req_cap);
 			}
 		}
@@ -7952,7 +7952,7 @@ static int active_load_balance_cpu_stop(void *data)
 					cpu_util(env.src_cpu);
 
 				req_cap = req_cap * capacity_margin
-						>> SCHED_CAPACITY_SHIFT;
+					/ capacity_orig_of(env.src_cpu);
 				cpufreq_sched_set_cap(env.src_cpu, req_cap);
 			}
 		}
@@ -7977,7 +7977,7 @@ out_unlock:
 			unsigned long req_cap = cpu_util(target_cpu);
 
 			req_cap = req_cap * capacity_margin
-					>> SCHED_CAPACITY_SHIFT;
+				/ capacity_orig_of(target_cpu);
 			cpufreq_sched_set_cap(target_cpu, req_cap);
 		}
 	}
@@ -8482,7 +8482,7 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 		if (capacity_curr < capacity_orig &&
 		    (capacity_curr * SCHED_LOAD_SCALE) <
 		    (cpu_util(cpu) * capacity_margin))
-			cpufreq_sched_set_cap(cpu, capacity_orig);
+			cpufreq_sched_set_cap(cpu, SCHED_CAPACITY_SCALE);
 	}
 }
 
-- 
1.9.1

