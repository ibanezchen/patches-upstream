From c4a3648fec763cc10bb4674f1f09aa3f8fc43da4 Mon Sep 17 00:00:00 2001
From: Steve Muckle <steve.muckle@linaro.org>
Date: Wed, 14 Oct 2015 15:43:20 -0700
Subject: [PATCH 14/19] sched/cpufreq_sched: extend cpufreq_sched API to
 multiple sched classes

Currently cpufreq_sched only effectively supports a single sched class,
since each capacity request overwrites the previous one.

Extend cpufreq_sched so that the RT and DEADLINE classes may also request
a specific CPU capacity. In addition, the DEADLINE class may request a
minimum capacity level as this is necessary to ensure the semantics of
the DEADLINE class are preserved.

The capacity requests from each scheduling class are summed and that sum
is requested from cpufreq. If there is a DEADLINE minimum capacity request
and that is greater than the sum, it is used instead.

Signed-off-by: Steve Muckle <steve.muckle@linaro.org>
---
 kernel/sched/cpufreq_sched.c | 37 ++++++++++++++++++-----------
 kernel/sched/fair.c          | 32 ++++++++++++++------------
 kernel/sched/sched.h         | 55 ++++++++++++++++++++++++++++++++++++++++----
 3 files changed, 91 insertions(+), 33 deletions(-)

diff --git a/kernel/sched/cpufreq_sched.c b/kernel/sched/cpufreq_sched.c
index c5c52b1..f68a89e 100644
--- a/kernel/sched/cpufreq_sched.c
+++ b/kernel/sched/cpufreq_sched.c
@@ -16,8 +16,10 @@
 
 #define THROTTLE_NSEC		50000000 /* 50ms default */
 
+extern unsigned int capacity_margin;
+
 static DEFINE_PER_CPU(unsigned long, pcpu_capacity);
-static DEFINE_PER_CPU(struct cpufreq_policy *, pcpu_policy);
+DEFINE_PER_CPU(struct sched_capacity_reqs, cpu_sched_capacity_reqs);
 
 /**
  * gov_data - per-policy data internal to the governor
@@ -144,7 +146,7 @@ static void cpufreq_sched_irq_work(struct irq_work *irq_work)
  * 1) this cpu did not the new maximum capacity for its frequency domain
  * 2) no change in cpu frequency is necessary to meet the new capacity request
  */
-void cpufreq_sched_set_cap(int cpu, unsigned long capacity)
+static void cpufreq_sched_set_cap(int cpu, unsigned long capacity)
 {
 	unsigned int freq_new, cpu_tmp;
 	struct cpufreq_policy *policy;
@@ -204,16 +206,25 @@ out:
 	return;
 }
 
-/**
- * cpufreq_sched_reset_capacity - interface to scheduler for resetting capacity
- *                                requests
- * @cpu: cpu whose capacity request has to be reset
- *
- * This _wont trigger_ any capacity update.
- */
-void cpufreq_sched_reset_cap(int cpu)
+void update_cpu_capacity_request(int cpu)
 {
-	per_cpu(pcpu_capacity, cpu) = 0;
+	unsigned long new_capacity;
+	struct sched_capacity_reqs *scr;
+
+	lockdep_assert_held(cpu_rq(cpu)->lock);
+
+	scr = &per_cpu(cpu_sched_capacity_reqs, cpu);
+
+	new_capacity = scr->cfs + scr->rt + scr->dl;
+	new_capacity = new_capacity * capacity_margin
+		/ SCHED_CAPACITY_SCALE;
+	if (new_capacity < scr->dl_min)
+		new_capacity = scr->dl_min;
+
+	if (new_capacity != scr->total) {
+		cpufreq_sched_set_cap(cpu, new_capacity);
+		scr->total = new_capacity;
+	}
 }
 
 static inline void set_sched_energy_freq(void)
@@ -241,10 +252,8 @@ static int cpufreq_sched_start(struct cpufreq_policy *policy)
 	}
 
 	/* initialize per-cpu data */
-	for_each_cpu(cpu, policy->cpus) {
+	for_each_cpu(cpu, policy->cpus)
 		per_cpu(pcpu_capacity, cpu) = 0;
-		per_cpu(pcpu_policy, cpu) = policy;
-	}
 
 	/*
 	 * Don't ask for freq changes at an higher rate than what
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index bf4da6f..351c4c7 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4118,7 +4118,7 @@ static inline void hrtick_update(struct rq *rq)
 }
 #endif
 
-static unsigned int capacity_margin = 1280; /* ~20% margin */
+unsigned int capacity_margin = 1280; /* ~20% margin */
 
 static bool cpu_overutilized(int cpu);
 static unsigned long cpu_util(int cpu);
@@ -4185,9 +4185,9 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		if (sched_energy_freq() && (task_new || task_wakeup)) {
 			unsigned long req_cap = cpu_util(cpu_of(rq));
 
-			req_cap = req_cap * capacity_margin
+			req_cap = req_cap * SCHED_CAPACITY_SCALE
 				/ capacity_orig_of(cpu_of(rq));
-			cpufreq_sched_set_cap(cpu_of(rq), req_cap);
+			set_cfs_cpu_capacity(cpu_of(rq), true, req_cap);
 		}
 	}
 	hrtick_update(rq);
@@ -4263,11 +4263,11 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 			unsigned long req_cap = cpu_util(cpu_of(rq));
 
 			if (rq->cfs.nr_running) {
-				req_cap = req_cap * capacity_margin
+				req_cap = req_cap * SCHED_CAPACITY_SCALE
 					/ capacity_orig_of(cpu_of(rq));
-				cpufreq_sched_set_cap(cpu_of(rq), req_cap);
+				set_cfs_cpu_capacity(cpu_of(rq), true, req_cap);
 			} else {
-				cpufreq_sched_reset_cap(cpu_of(rq));
+				set_cfs_cpu_capacity(cpu_of(rq), false, 0);
 			}
 		}
 	}
@@ -7550,9 +7550,9 @@ more_balance:
 		if (sched_energy_freq() && cur_ld_moved) {
 			unsigned long req_cap = cpu_util(env.src_cpu);
 
-			req_cap = req_cap * capacity_margin
+			req_cap = req_cap * SCHED_CAPACITY_SCALE
 				/ capacity_orig_of(env.src_cpu);
-			cpufreq_sched_set_cap(env.src_cpu, req_cap);
+			set_cfs_cpu_capacity(env.src_cpu, true, req_cap);
 		}
 
 		/*
@@ -7579,9 +7579,10 @@ more_balance:
 				unsigned long req_cap =
 					cpu_util(env.dst_cpu);
 
-				req_cap = req_cap * capacity_margin
+				req_cap = req_cap * SCHED_CAPACITY_SCALE
 					/ capacity_orig_of(env.dst_cpu);
-				cpufreq_sched_set_cap(env.dst_cpu, req_cap);
+				set_cfs_cpu_capacity(env.dst_cpu, true,
+						     req_cap);
 			}
 		}
 
@@ -7951,9 +7952,10 @@ static int active_load_balance_cpu_stop(void *data)
 				unsigned long req_cap =
 					cpu_util(env.src_cpu);
 
-				req_cap = req_cap * capacity_margin
+				req_cap = req_cap * SCHED_CAPACITY_SCALE
 					/ capacity_orig_of(env.src_cpu);
-				cpufreq_sched_set_cap(env.src_cpu, req_cap);
+				set_cfs_cpu_capacity(env.src_cpu, true,
+						     req_cap);
 			}
 		}
 		else
@@ -7976,9 +7978,9 @@ out_unlock:
 		if (sched_energy_freq()) {
 			unsigned long req_cap = cpu_util(target_cpu);
 
-			req_cap = req_cap * capacity_margin
+			req_cap = req_cap * SCHED_CAPACITY_SCALE
 				/ capacity_orig_of(target_cpu);
-			cpufreq_sched_set_cap(target_cpu, req_cap);
+			set_cfs_cpu_capacity(target_cpu, true, req_cap);
 		}
 	}
 
@@ -8482,7 +8484,7 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 		if (capacity_curr < capacity_orig &&
 		    (capacity_curr * SCHED_LOAD_SCALE) <
 		    (cpu_util(cpu) * capacity_margin))
-			cpufreq_sched_set_cap(cpu, SCHED_CAPACITY_SCALE);
+			set_cfs_cpu_capacity(cpu, true, SCHED_CAPACITY_SCALE);
 	}
 }
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9f6613d..daec870 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1451,13 +1451,60 @@ static inline bool sched_energy_freq(void)
 	return static_key_false(&__sched_energy_freq);
 }
 
+struct sched_capacity_reqs {
+	unsigned long cfs;
+	unsigned long rt;
+	unsigned long dl;
+	unsigned long dl_min;
+
+	unsigned long total;
+};
+
+extern unsigned int capacity_margin;
+
 #ifdef CONFIG_CPU_FREQ_GOV_SCHED
-void cpufreq_sched_set_cap(int cpu, unsigned long util);
-void cpufreq_sched_reset_cap(int cpu);
+DECLARE_PER_CPU(struct sched_capacity_reqs, cpu_sched_capacity_reqs);
+void update_cpu_capacity_request(int cpu);
+
+static inline void set_cfs_cpu_capacity(int cpu, bool request,
+					unsigned long capacity)
+{
+	per_cpu(cpu_sched_capacity_reqs, cpu).cfs = capacity;
+	if (request)
+		update_cpu_capacity_request(cpu);
+}
+
+static inline void set_rt_cpu_capacity(int cpu, bool request,
+				       unsigned long capacity)
+{
+	per_cpu(cpu_sched_capacity_reqs, cpu).rt = capacity;
+	if (request)
+		update_cpu_capacity_request(cpu);
+}
+
+static inline void set_dl_cpu_capacity(int cpu, bool request,
+				       unsigned long capacity,
+				       unsigned long min_capacity)
+{
+	struct sched_capacity_reqs *scr;
+
+	scr = &per_cpu(cpu_sched_capacity_reqs, cpu);
+	scr->dl = capacity;
+	scr->dl_min = min_capacity;
+
+	if (request)
+		update_cpu_capacity_request(cpu);
+}
 #else
-static inline void cpufreq_sched_set_cap(int cpu, unsigned long util)
+static inline void set_cfs_cpu_capacity(int cpu, bool request,
+					unsigned long capacity)
+{ }
+static inline void set_rt_cpu_capacity(int cpu, bool request,
+				       unsigned long capacity)
 { }
-static inline void cpufreq_sched_reset_cap(int cpu)
+static inline void set_dl_cpu_capacity(int cpu, bool request,
+				       unsigned long capacity,
+				       unsigned long min_capacity)
 { }
 #endif
 
-- 
1.9.1

