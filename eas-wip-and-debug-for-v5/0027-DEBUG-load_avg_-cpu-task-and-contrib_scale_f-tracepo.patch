From c96d05621be95815e2708f18760bba7b1fb705ab Mon Sep 17 00:00:00 2001
From: Juri Lelli <juri.lelli@arm.com>
Date: Thu, 12 Feb 2015 17:55:47 +0000
Subject: [PATCH 27/38] DEBUG: load_avg_{cpu,task} and contrib_scale_f
 tracepoints

Change-Id: I923425a231cbbc5dfac86fc3e2a90459761bd2ea
Signed-off-by: Juri Lelli <juri.lelli@arm.com>
---
 include/trace/events/sched.h | 86 ++++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/fair.c          |  8 +++++
 2 files changed, 94 insertions(+)

diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
index d57a575..f998e4b 100644
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@ -554,6 +554,92 @@ TRACE_EVENT(sched_wake_idle_without_ipi,
 
 	TP_printk("cpu=%d", __entry->cpu)
 );
+
+TRACE_EVENT(sched_contrib_scale_f,
+
+	TP_PROTO(int cpu, unsigned long freq_scale_factor,
+		 unsigned long cpu_scale_factor),
+
+	TP_ARGS(cpu, freq_scale_factor, cpu_scale_factor),
+
+	TP_STRUCT__entry(
+		__field(int, cpu)
+		__field(unsigned long, freq_scale_factor)
+		__field(unsigned long, cpu_scale_factor)
+	),
+
+	TP_fast_assign(
+		__entry->cpu = cpu;
+		__entry->freq_scale_factor = freq_scale_factor;
+		__entry->cpu_scale_factor = cpu_scale_factor;
+	),
+
+	TP_printk("cpu=%d freq_scale_factor=%lu cpu_scale_factor=%lu",
+		  __entry->cpu, __entry->freq_scale_factor,
+		  __entry->cpu_scale_factor)
+);
+
+/*
+ * Tracepoint for accounting sched averages for tasks.
+ */
+TRACE_EVENT(sched_load_avg_task,
+
+	TP_PROTO(struct task_struct *tsk, struct sched_avg *avg),
+
+	TP_ARGS(tsk, avg),
+
+	TP_STRUCT__entry(
+		__array( char,	comm,	TASK_COMM_LEN		)
+		__field( pid_t,	pid				)
+		__field( unsigned long,	load			)
+		__field( unsigned long,	utilization		)
+		__field( unsigned int,	runnable_avg_sum	)
+		__field( unsigned int,	running_avg_sum		)
+		__field( unsigned int,	avg_period		)
+	),
+
+	TP_fast_assign(
+		memcpy(__entry->comm, tsk->comm, TASK_COMM_LEN);
+		__entry->pid			= tsk->pid;
+		__entry->load			= avg->load_avg_contrib;
+		__entry->utilization		= avg->utilization_avg_contrib;
+		__entry->runnable_avg_sum	= avg->runnable_avg_sum;
+		__entry->running_avg_sum	= avg->running_avg_sum;
+		__entry->avg_period		= avg->avg_period;
+	),
+
+	TP_printk("comm=%s pid=%d load=%lu utilization=%lu runnable_avg_sum=%u"
+		  " running_avg_sum=%u avg_period=%u",
+		  __entry->comm, __entry->pid, __entry->load, __entry->utilization,
+		  (unsigned int)__entry->runnable_avg_sum,
+		  (unsigned int)__entry->running_avg_sum,
+		  (unsigned int)__entry->avg_period)
+);
+
+/*
+ * Tracepoint for accounting sched averages for cpus.
+ */
+TRACE_EVENT(sched_load_avg_cpu,
+
+	TP_PROTO(int cpu, struct cfs_rq *cfs_rq),
+
+	TP_ARGS(cpu, cfs_rq),
+
+	TP_STRUCT__entry(
+		__field( int,	cpu				)
+		__field( unsigned long,	load			)
+		__field( unsigned long,	utilization		)
+	),
+
+	TP_fast_assign(
+		__entry->cpu			= cpu;
+		__entry->load			= cfs_rq->runnable_load_avg;
+		__entry->utilization		= cfs_rq->utilization_load_avg;
+	),
+
+	TP_printk("cpu=%d load=%lu utilization=%lu",
+		  __entry->cpu, __entry->load, __entry->utilization)
+);
 #endif /* _TRACE_SCHED_H */
 
 /* This part must be outside protection */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 053a57c..7264623 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2557,6 +2557,9 @@ static __always_inline int __update_entity_runnable_avg(u64 now, int cpu,
 	unsigned long scale_freq = arch_scale_freq_capacity(NULL, cpu);
 	unsigned long scale_cpu = arch_scale_cpu_capacity(NULL, cpu);
 
+	trace_sched_contrib_scale_f(cpu, scale_freq, scale_cpu);
+
+
 	delta = now - sa->last_runnable_update;
 	/*
 	 * This should only happen when time goes backwards, which it
@@ -2855,6 +2858,9 @@ static inline void update_entity_load_avg(struct sched_entity *se,
 	contrib_delta = __update_entity_load_avg_contrib(se);
 	utilization_delta = __update_entity_utilization_avg_contrib(se);
 
+	if (entity_is_task(se))
+		trace_sched_load_avg_task(task_of(se), &se->avg);
+
 	if (!update_cfs_rq)
 		return;
 
@@ -2866,6 +2872,8 @@ static inline void update_entity_load_avg(struct sched_entity *se,
 		subtract_utilization_blocked_contrib(cfs_rq,
 							-utilization_delta);
 	}
+
+	trace_sched_load_avg_cpu(cpu, cfs_rq);
 }
 
 /*
-- 
1.9.1

